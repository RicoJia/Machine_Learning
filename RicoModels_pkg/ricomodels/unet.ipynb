{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notes:\n",
    "- Technically, UNet can be used on images of various input images. Would you recommend doing the same transforms? \n",
    "- What does torch.modulelist do? What is it similar to in tensor flow?\n",
    "    - An `nn.module` is usually a layer / group of layers. `torch.modulelist` is a container for that, and it can be indexed like a regular list. But in the meantime these are layers are registered in the model for calls like `model.to()`, `model.train()`, `model.eval()`\n",
    "\n",
    "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d) - TODO\n",
    "    ```python\n",
    "    conv_transpose = nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=2, stride=2, padding=0)\n",
    "    input_tensor = torch.randn(1, 3, 5, 5)  # 1 batch, 3 channels, 5x5 image\n",
    "    output_tensor = conv_transpose(input_tensor)\n",
    "    output_tensor.shape\n",
    "    ```\n",
    "- How does concat work? horizontally: `torch.cat((x, x, x), 1)`, vertically: `torch.cat((x, x, x), 0)`\n",
    "\n",
    "- How to get model summary in pytorch? \n",
    "```\n",
    "pip install torchsummary\n",
    "from torchsummary import summary\n",
    "# Assuming `model` is your neural network\n",
    "summary(model, input_size=(3, 224, 224))  # For an input image of size 224x224 with 3 channels (RGB)\n",
    "```\n",
    "- Here, we need to do `crop()` actually. The best way is `from torchvision.transforms import CenterCrop`\n",
    "\n",
    "- increase dimension at dim=0: `new_t = t.unsqueeze(0)`\n",
    "\n",
    "- What do you do when the final output is `torch.Size([1, 16, 216, 216])`, while your input is `torch.Size([1, 3, 256, 256])`\n",
    "\n",
    "- my labels are int8, I'm going to train my network. What data type should my output have\n",
    "\n",
    "- Interpolation for image downsizing, and final output outsizing is important. We choose Nearest neighbor interpolation. Because continuous interpolation or spline does not make sense for image segmentation\n",
    "\n",
    "- `squeeze()`, `unsqueeze()`\n",
    "\n",
    "- Do I need to apply trasnforms in dataset object? TODO\n",
    "\n",
    "- Ignore_index should be 0, not 21. This caused loss not to penalize 0 and the model got into a huge plateau area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2, CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from functools import cached_property\n",
    "\n",
    "DATA_DIR='./data'\n",
    "BATCH_SIZE = 16\n",
    "IGNORE_INDEX = 0\n",
    "\n",
    "def replace_tensor_val(tensor, a, b):\n",
    "    tensor[tensor==a] = b\n",
    "    return tensor\n",
    "\n",
    "image_seg_transforms = transforms.Compose([\n",
    "   v2.Resize((256, 256)),\n",
    "    # Becareful because you want to rotate your transforms by the same amount\n",
    "    # v2.RandomHorizontalFlip(),\n",
    "    # v2.RandomVerticalFlip(),\n",
    "    # v2.RandomRotation(degrees=15),\n",
    "\n",
    "   v2.ToTensor(),\n",
    "   v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_seg_transforms = transforms.Compose([\n",
    "    v2.Resize((256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "    v2.PILToTensor(),\n",
    "    v2.Lambda(lambda tensor: tensor.squeeze()),\n",
    "    v2.Lambda(lambda x: replace_tensor_val(x.long(), 255, IGNORE_INDEX)),\n",
    "])\n",
    "\n",
    "class VOCSegmentationClass(Dataset):\n",
    "   def __init__(self, image_set): \n",
    "        # Load PASCAL VOC 2007 dataset for segmentation\n",
    "        self._dataset = datasets.VOCSegmentation(\n",
    "            root=DATA_DIR,  # Specify where to store the data\n",
    "            year='2007',    # Specify the year of the dataset (2007 in this case)\n",
    "            image_set=image_set,  # You can use 'train', 'val', or 'trainval'\n",
    "            download=True,  # Automatically download if not available\n",
    "            transform=image_seg_transforms,  # Apply transformations to the images\n",
    "            target_transform=target_seg_transforms  # Apply transformations to the masks\n",
    "        )\n",
    "        self._classes = set()\n",
    "   @cached_property\n",
    "   def classes(self):\n",
    "       if len(self._classes) == 0: \n",
    "           for image, target in self._dataset:\n",
    "            self._classes.update(torch.unique(target).tolist())\n",
    "       return self._classes\n",
    "   def __getitem__(self, index): \n",
    "       # return an image and a label. In this case, a label is an image with int8 values\n",
    "       return self._dataset[index]\n",
    "       # TODO: more transforms?\n",
    "   def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "train_dataset = VOCSegmentationClass(image_set='train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_dataset = VOCSegmentationClass(image_set='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_and_target(image, target=None, labels=None):\n",
    "    # # See torch.Size([3, 281, 500]) torch.Size([1, 281, 500])\n",
    "    # # print(image.shape, target.shape)\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    # Making channels the last dimension\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.title('image')\n",
    "\n",
    "    if target is not None:\n",
    "        plt.subplot(1,3,2)\n",
    "        # Making channels the last dimension\n",
    "        plt.imshow(target)\n",
    "        plt.title('mask')\n",
    "\n",
    "    if labels is not None:\n",
    "        plt.subplot(1,3,3)\n",
    "        # Making channels the last dimension\n",
    "        plt.imshow(labels)\n",
    "        plt.title('labels')\n",
    "    \n",
    "    # See tensor([  0,   1,  15, 255], dtype=torch.uint8)\n",
    "    plt.show()\n",
    "    \n",
    "print(\"classes: \", train_dataset.classes)\n",
    "for image, target in train_dataset:\n",
    "    visualize_image_and_target(image, labels=target)\n",
    "    break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a regular conv block\n",
    "from torch import nn as nn\n",
    "from collections import deque\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # This should include the bottleneck.\n",
    "        self._layers = nn.ModuleList([ConvBlock(in_channels[i], in_channels[i+1]) for i in range(len(in_channels) - 1)])\n",
    "        self._maxpool = nn.MaxPool2d(2, stride=2)\n",
    "    def forward(self, x):\n",
    "        # returns unpooled output from each block: \n",
    "        # [intermediate results ... ], but we don't want to return \n",
    "        intermediate_outputs = deque([])\n",
    "        for i in range(len(self._layers) - 1):\n",
    "            x = self._layers[i](x)\n",
    "            intermediate_outputs.appendleft(x)\n",
    "            x = self._maxpool(x)\n",
    "        x = self._layers[-1](x)\n",
    "        return x, intermediate_outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self._upward_conv_blocks = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels = channels[i], out_channels = channels[i+1], \n",
    "                kernel_size=2, stride=2\n",
    "            ) for i in range(len(channels) - 1)\n",
    "        ])\n",
    "        # Then, there's a concat step in between\n",
    "        self._conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels= channels[i], out_channels=channels[i+1]) \n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, skip_inputs):\n",
    "        if len(skip_inputs) != len(self._conv_blocks):\n",
    "            raise ValueError(\"Please check implementation. Length of skip inputs and _conv_blocks should be the same!\",\n",
    "                             f\"skip inputs, blocks inputs: {len(skip_inputs), len(self._conv_blocks)}\")\n",
    "        # x is smaller than skip inputs, because there's no padding in the conv layers\n",
    "        for skip_input, up_block, conv_block in zip(skip_inputs, self._upward_conv_blocks, self._conv_blocks):\n",
    "            # print(\"x shape before upsampling: \", x.shape)\n",
    "            x = up_block(x)\n",
    "            # print(skip_input.shape, x.shape)\n",
    "            # TODO: here's a small detail. The paper didn't specify if we want to append or prepend. This might cause trouble\n",
    "            skip_input = self.crop(skip_input=skip_input, x=x)\n",
    "            x = torch.cat((skip_input, x), 1)\n",
    "            # TODO, I'm really not sure if we need to crop. \n",
    "            x = conv_block(x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, skip_input, x):\n",
    "        _, _, H, W = x.shape\n",
    "        return CenterCrop((H,W))(skip_input)\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        encoder_in_channels= [3, 32, 64, 128]    # bottleneck is 128\n",
    "        decoder_channels = [128, 64, 32] #?\n",
    "        self._encoder = Encoder(in_channels=encoder_in_channels)\n",
    "        self._decoder = Decoder(channels=decoder_channels)\n",
    "        # 1x1\n",
    "        self._head = nn.Conv2d(in_channels=decoder_channels[-1], out_channels=class_num, kernel_size=1)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        x, intermediate = self._encoder(x)\n",
    "        output = self._decoder(x, intermediate)\n",
    "        output = self._head(output)\n",
    "        output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "        return output\n",
    "\n",
    "    def _init_weight(self):\n",
    "        with torch.no_grad():\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                    print(f\"{type(m)}, he initialization\")\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    print(f\"{type(m)}, const init\")\n",
    "                    \n",
    "\n",
    "def forward_pass_poc():\n",
    "    image, target = train_dataset[0]\n",
    "    print(target.shape)\n",
    "    class_num = len(train_dataset.classes)\n",
    "    image = image.unsqueeze(0)\n",
    "    _, _, H, W = image.shape\n",
    "    enc = Encoder([3, 16, 32, 64])\n",
    "    # # print(image.shape)\n",
    "    x, intermediate_outputs = enc.forward(image)\n",
    "    dec = Decoder(channels=[64, 32, 16])\n",
    "    # torch.Size([1, 16, 216, 216])\n",
    "    output = dec(x, intermediate_outputs)\n",
    "    # 1x1\n",
    "    head = nn.Conv2d(\n",
    "        in_channels=16,\n",
    "        out_channels=class_num,\n",
    "        kernel_size=1,\n",
    "    )\n",
    "    output = head(output)\n",
    "    output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "    print(output.shape)\n",
    "forward_pass_poc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, device, visualize: bool = False):\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        # TODO I AM ITERATING OVER TRAIN_LOADER, SO I'M MORE SURE\n",
    "        for inputs_test, labels_test in test_loader:\n",
    "            inputs_test = inputs_test.to(device)\n",
    "            labels_test = labels_test.to(device)\n",
    "            outputs_test = model(inputs_test)\n",
    "            _, predicted_test = outputs_test.max(1)\n",
    "            mask = (labels_test != 21)\n",
    "            local_total = mask.sum().item()\n",
    "            local_correct = (predicted_test.eq(labels_test) & mask).sum().item()\n",
    "            total_test += local_total\n",
    "            correct_test += local_correct\n",
    "\n",
    "            if visualize:\n",
    "                #TODO Remember to remove\n",
    "                print(f'Rico: predicted test acc {100. * local_correct/local_total}%')\n",
    "                for img, pred, lab in zip(inputs_test, predicted_test, labels_test):\n",
    "                    print(\"pred uniq: \", torch.unique(pred), \"lab uniq: \", torch.unique(lab))\n",
    "                    visualize_image_and_target(img.cpu(), pred.cpu(), lab.cpu())\n",
    "\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "    print(f'Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from torch import optim\n",
    "\n",
    "# Define the training function\n",
    "MODEL_PATH = 'unet_pascal.pth'\n",
    "ACCUMULATION_STEPS = 8\n",
    "\n",
    "# Check against example\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        start = time.time()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] ')\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # This is because torch.nn.CrossEntropyLoss(reduction='mean') is true, so to simulate a larger batch, we need to further divide\n",
    "            # print(f\"output: {outputs.dtype}, labels: {labels.dtype}\")\n",
    "            loss = criterion(outputs, labels)/ACCUMULATION_STEPS\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            if (i+1)%ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # break #TODO\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            # print(predicted.shape)\n",
    "            mask = (labels != 21)\n",
    "            total_train += mask.sum().item()\n",
    "            # print((predicted == labels).sum().item(), ((predicted == labels) & mask).sum().item())\n",
    "            # print(mask.sum().item())\n",
    "            \n",
    "            correct_train += ((predicted == labels) & mask).sum().item()\n",
    "\n",
    "        # adjust after every epoch\n",
    "        scheduler.step()  # TODO: disabled for Adam optimizer\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100. * correct_train / total_train\n",
    "        print(\"correct train: \", correct_train, \" total train: \", total_train)\n",
    "        end = time.time()\n",
    "        \n",
    "        print(\"elapsed: \", end-start)\n",
    "\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"epoch: {epoch}, saved the model. \"\n",
    "              f'Train Loss: {epoch_loss:.4f} '\n",
    "              f'Train Acc: {epoch_acc:.2f}% ')\n",
    "    # eval_model(model, test_loader=test_dataloader, device=device) \n",
    "    print('Training complete')\n",
    "    return model\n",
    "\n",
    "model = UNet(class_num = len(train_dataset.classes))\n",
    "\n",
    "# TODO: let's see this imbalance dataset\n",
    "zero_class_weight = 0.01\n",
    "other_class_weight = (1-zero_class_weight)/(len(train_dataset.classes)-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "weight_decay = 0.0001\n",
    "# momentum=0.9\n",
    "learning_rate=0.001\n",
    "num_epochs=70\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 50], gamma=0.1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=False, map_location=device))\n",
    "    print(\"loaded model\")\n",
    "model.to(device)\n",
    "\n",
    "# model = train_model(model, train_dataloader, test_dataloader, criterion, optimizer, scheduler,\n",
    "#                     num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (3, 356, 356), summary_mode=True)\n",
    "\n",
    "def calculate_average_weights(model):\n",
    "    total_sum = 0\n",
    "    total_elements = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            weight_mean = param.mean().item()\n",
    "            total_sum += param.sum().item()\n",
    "            total_elements += param.numel()\n",
    "            print(f\"Layer: {name} | Average Weight: {weight_mean:.6f}\")\n",
    "    \n",
    "    overall_average = total_sum / total_elements if total_elements > 0 else 0\n",
    "    print(f\"Overall Average Weight in the Network: {overall_average:.6f}\")\n",
    "\n",
    "calculate_average_weights(model)\n",
    "\n",
    "eval_model(model, test_dataloader, device=device, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
