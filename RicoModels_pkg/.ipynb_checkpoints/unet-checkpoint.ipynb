{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notes:\n",
    "- Technically, UNet can be used on images of various input images. Would you recommend doing the same transforms? \n",
    "- What does torch.modulelist do? What is it similar to in tensor flow?\n",
    "    - An `nn.module` is usually a layer / group of layers. `torch.modulelist` is a container for that, and it can be indexed like a regular list. But in the meantime these are layers are registered in the model for calls like `model.to()`, `model.train()`, `model.eval()`\n",
    "\n",
    "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d) - TODO\n",
    "    ```python\n",
    "    conv_transpose = nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=2, stride=2, padding=0)\n",
    "    input_tensor = torch.randn(1, 3, 5, 5)  # 1 batch, 3 channels, 5x5 image\n",
    "    output_tensor = conv_transpose(input_tensor)\n",
    "    output_tensor.shape\n",
    "    ```\n",
    "- How does concat work? horizontally: `torch.cat((x, x, x), 1)`, vertically: `torch.cat((x, x, x), 0)`\n",
    "\n",
    "- How to get model summary in pytorch? \n",
    "```\n",
    "pip install torchsummary\n",
    "from torchsummary import summary\n",
    "# Assuming `model` is your neural network\n",
    "summary(model, input_size=(3, 224, 224))  # For an input image of size 224x224 with 3 channels (RGB)\n",
    "```\n",
    "- Here, we need to do `crop()` actually. The best way is `from torchvision.transforms import CenterCrop`\n",
    "\n",
    "- increase dimension at dim=0: `new_t = t.unsqueeze(0)`\n",
    "\n",
    "- What do you do when the final output is `torch.Size([1, 16, 216, 216])`, while your input is `torch.Size([1, 3, 256, 256])`\n",
    "\n",
    "- my labels are int8, I'm going to train my network. What data type should my output have\n",
    "\n",
    "- Interpolation for image downsizing, and final output outsizing is important. We choose Nearest neighbor interpolation. Because continuous interpolation or spline does not make sense for image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rico/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2, CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from functools import cached_property\n",
    "\n",
    "DATA_DIR='./data'\n",
    "BATCH_SIZE = 4\n",
    "IGNORE_INDEX = 21\n",
    "\n",
    "def replace_tensor_val(tensor, a, b):\n",
    "    tensor[tensor==a] = b\n",
    "    return tensor\n",
    "\n",
    "image_seg_transforms = transforms.Compose([\n",
    "   v2.Resize((256, 256)),\n",
    "   v2.ToTensor(),\n",
    "   v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_seg_transforms = transforms.Compose([\n",
    "    v2.Resize((256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "    v2.PILToTensor(),\n",
    "    v2.Lambda(lambda tensor: tensor.squeeze()),\n",
    "    v2.Lambda(lambda x: replace_tensor_val(x.long(), 255, 21)),\n",
    "])\n",
    "\n",
    "class VOCSegmentationClass(Dataset):\n",
    "   def __init__(self, image_set): \n",
    "        # Load PASCAL VOC 2007 dataset for segmentation\n",
    "        self._dataset = datasets.VOCSegmentation(\n",
    "            root=DATA_DIR,  # Specify where to store the data\n",
    "            year='2007',    # Specify the year of the dataset (2007 in this case)\n",
    "            image_set=image_set,  # You can use 'train', 'val', or 'trainval'\n",
    "            download=True,  # Automatically download if not available\n",
    "            transform=image_seg_transforms,  # Apply transformations to the images\n",
    "            target_transform=target_seg_transforms  # Apply transformations to the masks\n",
    "        )\n",
    "        self._classes = set()\n",
    "   @cached_property\n",
    "   def classes(self):\n",
    "       if len(self._classes) == 0: \n",
    "           for image, target in self._dataset:\n",
    "            self._classes.update(torch.unique(target).tolist())\n",
    "       return self._classes\n",
    "   def __getitem__(self, index): \n",
    "       # return an image and a label. In this case, a label is an image with int8 values\n",
    "       return self._dataset[index]\n",
    "       # TODO: more transforms?\n",
    "   def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "train_dataset = VOCSegmentationClass(image_set='train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_dataset = VOCSegmentationClass(image_set='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"classes: \", train_dataset.classes)\n",
    "# for image, target in train_dataset:\n",
    "# #     # To see what our data looks like\n",
    "# #     # # See torch.Size([3, 281, 500]) torch.Size([1, 281, 500])\n",
    "#     # # print(image.shape, target.shape)\n",
    "\n",
    "# #     plt.subplot(1,2,1)\n",
    "# #     # Making channels the last dimension\n",
    "# #     plt.imshow(image.permute(1,2,0))\n",
    "# #     plt.title('image')\n",
    "\n",
    "# #     plt.subplot(1,2,2)\n",
    "# #     # Making channels the last dimension\n",
    "# #     plt.imshow(target.permute(1,2,0))\n",
    "# #     plt.title('mask')\n",
    "# #     # See tensor([  0,   1,  15, 255], dtype=torch.uint8)\n",
    "# #     print(\"unique: \", torch.unique(target[0]))\n",
    "# #     plt.show()\n",
    "# #     break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 22, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# This is a regular conv block\n",
    "from torch import nn as nn\n",
    "from collections import deque\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # This should include the bottleneck.\n",
    "        self._layers = nn.ModuleList([ConvBlock(in_channels[i], in_channels[i+1]) for i in range(len(in_channels) - 1)])\n",
    "        self._maxpool = nn.MaxPool2d(2, stride=2)\n",
    "    def forward(self, x):\n",
    "        # returns unpooled output from each block: \n",
    "        # [intermediate results ... ], but we don't want to return \n",
    "        intermediate_outputs = deque([])\n",
    "        for i in range(len(self._layers) - 1):\n",
    "            x = self._layers[i](x)\n",
    "            intermediate_outputs.appendleft(x)\n",
    "            x = self._maxpool(x)\n",
    "        x = self._layers[-1](x)\n",
    "        return x, intermediate_outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self._upward_conv_blocks = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels = channels[i], out_channels = channels[i+1], \n",
    "                kernel_size=2, stride=2\n",
    "            ) for i in range(len(channels) - 1)\n",
    "        ])\n",
    "        # Then, there's a concat step in between\n",
    "        self._conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels= channels[i], out_channels=channels[i+1]) \n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, skip_inputs):\n",
    "        if len(skip_inputs) != len(self._conv_blocks):\n",
    "            raise ValueError(\"Please check implementation. Length of skip inputs and _conv_blocks should be the same!\",\n",
    "                             f\"skip inputs, blocks inputs: {len(skip_inputs), len(self._conv_blocks)}\")\n",
    "        # x is smaller than skip inputs, because there's no padding in the conv layers\n",
    "        for skip_input, up_block, conv_block in zip(skip_inputs, self._upward_conv_blocks, self._conv_blocks):\n",
    "            # print(\"x shape before upsampling: \", x.shape)\n",
    "            x = up_block(x)\n",
    "            # print(skip_input.shape, x.shape)\n",
    "            # TODO: here's a small detail. The paper didn't specify if we want to append or prepend. This might cause trouble\n",
    "            skip_input = self.crop(skip_input=skip_input, x=x)\n",
    "            x = torch.cat((skip_input, x), 1)\n",
    "            # TODO, I'm really not sure if we need to crop. \n",
    "            x = conv_block(x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, skip_input, x):\n",
    "        _, _, H, W = x.shape\n",
    "        return CenterCrop((H,W))(skip_input)\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        encoder_in_channels= [3, 16, 32, 64]    # bottleneck is 128\n",
    "        decoder_channels = [64, 32, 16] #?\n",
    "        self._encoder = Encoder(in_channels=encoder_in_channels)\n",
    "        self._decoder = Decoder(channels=decoder_channels)\n",
    "        # 1x1\n",
    "        self._head = nn.Conv2d(in_channels=decoder_channels[-1], out_channels=class_num, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        x, intermediate = self._encoder(x)\n",
    "        output = self._decoder(x, intermediate)\n",
    "        output = self._head(output)\n",
    "        output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "        return output\n",
    "\n",
    "def forward_pass_poc():\n",
    "    image, target = train_dataset[0]\n",
    "    print(target.shape)\n",
    "    class_num = len(train_dataset.classes)\n",
    "    image = image.unsqueeze(0)\n",
    "    _, _, H, W = image.shape\n",
    "    enc = Encoder([3, 16, 32, 64])\n",
    "    # # print(image.shape)\n",
    "    x, intermediate_outputs = enc.forward(image)\n",
    "    dec = Decoder(channels=[64, 32, 16])\n",
    "    # torch.Size([1, 16, 216, 216])\n",
    "    output = dec(x, intermediate_outputs)\n",
    "    # 1x1\n",
    "    head = nn.Conv2d(\n",
    "        in_channels=16,\n",
    "        out_channels=class_num,\n",
    "        kernel_size=1,\n",
    "    )\n",
    "    output = head(output)\n",
    "    output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "    print(output.shape)\n",
    "forward_pass_poc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70] \n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n",
      "output: torch.float32, labels: torch.int64\n",
      "labes shape:  torch.Size([4, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 94\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabes shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# This is because torch.nn.CrossEntropyLoss(reduction='mean') is true, so to simulate a larger batch, we need to further divide\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 85\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     84\u001b[0m     _, _, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 85\u001b[0m     x, intermediate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder(x, intermediate)\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head(output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 35\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[i](x)\n\u001b[1;32m     34\u001b[0m     intermediate_outputs\u001b[38;5;241m.\u001b[39mappendleft(x)\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, intermediate_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from torch import optim\n",
    "\n",
    "# Define the training function\n",
    "MODEL_PATH = 'unet_pascal.pth'\n",
    "ACCUMULATION_STEPS = 8\n",
    "# Check against example\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        start = time.time()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] ')\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # This is because torch.nn.CrossEntropyLoss(reduction='mean') is true, so to simulate a larger batch, we need to further divide\n",
    "            print(f\"output: {outputs.dtype}, labels: {labels.dtype}\")\n",
    "            loss = criterion(outputs, labels)/ACCUMULATION_STEPS\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            if (i+1)%ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # break #TODO\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            mask = labels != 21\n",
    "            total_train = mask.sum().item()\n",
    "            correct_train += (predicted.eq(labels) & mask).sum().item()\n",
    "\n",
    "        # adjust after every epoch\n",
    "        # scheduler.step()  # TODO: disabled for Adam optimizer\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100. * correct_train / total_train\n",
    "        print(\"correct train: \", correct_train, \" total train: \", total_train)\n",
    "\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"epoch: {epoch}, saved the model. \"\n",
    "              f'Train Loss: {epoch_loss:.4f} '\n",
    "              f'Train Acc: {epoch_acc:.2f}% ')\n",
    "        \n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        # TODO I AM ITERATING OVER TRAIN_LOADER, SO I'M MORE SURE\n",
    "        for inputs_test, labels_test in test_loader:\n",
    "            inputs_test = inputs_test.to(device)\n",
    "            labels_test = labels_test.to(device)\n",
    "            outputs_test = model(inputs_test)\n",
    "            _, predicted_test = outputs_test.max(1)\n",
    "            total_test += labels_test.size(0)\n",
    "            correct_test += predicted_test.eq(labels_test).sum().item()\n",
    "\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "    # Adjust learning rate\n",
    "    end = time.time()\n",
    "    print(\"elapsed: \", end-start)\n",
    "    print(f'Test Acc: {test_acc:.2f}%')\n",
    "    print('Training complete')\n",
    "    return model\n",
    "\n",
    "model = UNet(class_num = len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "weight_decay = 0.0001\n",
    "# momentum=0.9\n",
    "learning_rate=0.001\n",
    "num_epochs=70\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=False, map_location=device))\n",
    "    print(\"loaded model\")\n",
    "model.to(device)\n",
    "\n",
    "model = train_model(model, train_dataloader, test_dataloader, criterion, optimizer, scheduler,\n",
    "                    num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
