{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notes:\n",
    "- Technically, UNet can be used on images of various input images. Would you recommend doing the same transforms? \n",
    "- What does torch.modulelist do? What is it similar to in tensor flow?\n",
    "    - An `nn.module` is usually a layer / group of layers. `torch.modulelist` is a container for that, and it can be indexed like a regular list. But in the meantime these are layers are registered in the model for calls like `model.to()`, `model.train()`, `model.eval()`\n",
    "\n",
    "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d) - TODO\n",
    "    ```python\n",
    "    conv_transpose = nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=2, stride=2, padding=0)\n",
    "    input_tensor = torch.randn(1, 3, 5, 5)  # 1 batch, 3 channels, 5x5 image\n",
    "    output_tensor = conv_transpose(input_tensor)\n",
    "    output_tensor.shape\n",
    "    ```\n",
    "- How does concat work? horizontally: `torch.cat((x, x, x), 1)`, vertically: `torch.cat((x, x, x), 0)`\n",
    "\n",
    "- How to get model summary in pytorch? \n",
    "```\n",
    "pip install torchsummary\n",
    "from torchsummary import summary\n",
    "# Assuming `model` is your neural network\n",
    "summary(model, input_size=(3, 224, 224))  # For an input image of size 224x224 with 3 channels (RGB)\n",
    "```\n",
    "- Here, we need to do `crop()` actually. The best way is `from torchvision.transforms import CenterCrop`\n",
    "\n",
    "- increase dimension at dim=0: `new_t = t.unsqueeze(0)`\n",
    "\n",
    "- What do you do when the final output is `torch.Size([1, 16, 216, 216])`, while your input is `torch.Size([1, 3, 256, 256])`\n",
    "\n",
    "- my labels are int8, I'm going to train my network. What data type should my output have\n",
    "\n",
    "- Interpolation for image downsizing, and final output outsizing is important. We choose Nearest neighbor interpolation. Because continuous interpolation or spline does not make sense for image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rico/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2, CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from functools import cached_property\n",
    "\n",
    "DATA_DIR='./data'\n",
    "BATCH_SIZE = 16\n",
    "IGNORE_INDEX = 21\n",
    "\n",
    "def replace_tensor_val(tensor, a, b):\n",
    "    tensor[tensor==a] = b\n",
    "    return tensor\n",
    "\n",
    "image_seg_transforms = transforms.Compose([\n",
    "   v2.Resize((256, 256)),\n",
    "   v2.ToTensor(),\n",
    "   v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_seg_transforms = transforms.Compose([\n",
    "    v2.Resize((256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "    v2.PILToTensor(),\n",
    "    v2.Lambda(lambda tensor: tensor.squeeze()),\n",
    "    v2.Lambda(lambda x: replace_tensor_val(x.long(), 255, 21)),\n",
    "])\n",
    "\n",
    "class VOCSegmentationClass(Dataset):\n",
    "   def __init__(self, image_set): \n",
    "        # Load PASCAL VOC 2007 dataset for segmentation\n",
    "        self._dataset = datasets.VOCSegmentation(\n",
    "            root=DATA_DIR,  # Specify where to store the data\n",
    "            year='2007',    # Specify the year of the dataset (2007 in this case)\n",
    "            image_set=image_set,  # You can use 'train', 'val', or 'trainval'\n",
    "            download=True,  # Automatically download if not available\n",
    "            transform=image_seg_transforms,  # Apply transformations to the images\n",
    "            target_transform=target_seg_transforms  # Apply transformations to the masks\n",
    "        )\n",
    "        self._classes = set()\n",
    "   @cached_property\n",
    "   def classes(self):\n",
    "       if len(self._classes) == 0: \n",
    "           for image, target in self._dataset:\n",
    "            self._classes.update(torch.unique(target).tolist())\n",
    "       return self._classes\n",
    "   def __getitem__(self, index): \n",
    "       # return an image and a label. In this case, a label is an image with int8 values\n",
    "       return self._dataset[index]\n",
    "       # TODO: more transforms?\n",
    "   def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "train_dataset = VOCSegmentationClass(image_set='train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_dataset = VOCSegmentationClass(image_set='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_and_target(image, target):\n",
    "    # # See torch.Size([3, 281, 500]) torch.Size([1, 281, 500])\n",
    "    # # print(image.shape, target.shape)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    # Making channels the last dimension\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.title('image')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    # Making channels the last dimension\n",
    "    plt.imshow(target.permute(1,2,0))\n",
    "    plt.title('mask')\n",
    "    # See tensor([  0,   1,  15, 255], dtype=torch.uint8)\n",
    "    print(\"unique: \", torch.unique(target[0]))\n",
    "    plt.show()\n",
    "    \n",
    "# # print(\"classes: \", train_dataset.classes)\n",
    "# for image, target in train_dataset:\n",
    "    # visualize_image_and_target(image, target)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n",
      "torch.Size([1, 22, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# This is a regular conv block\n",
    "from torch import nn as nn\n",
    "from collections import deque\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # This should include the bottleneck.\n",
    "        self._layers = nn.ModuleList([ConvBlock(in_channels[i], in_channels[i+1]) for i in range(len(in_channels) - 1)])\n",
    "        self._maxpool = nn.MaxPool2d(2, stride=2)\n",
    "    def forward(self, x):\n",
    "        # returns unpooled output from each block: \n",
    "        # [intermediate results ... ], but we don't want to return \n",
    "        intermediate_outputs = deque([])\n",
    "        for i in range(len(self._layers) - 1):\n",
    "            x = self._layers[i](x)\n",
    "            intermediate_outputs.appendleft(x)\n",
    "            x = self._maxpool(x)\n",
    "        x = self._layers[-1](x)\n",
    "        return x, intermediate_outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self._upward_conv_blocks = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels = channels[i], out_channels = channels[i+1], \n",
    "                kernel_size=2, stride=2\n",
    "            ) for i in range(len(channels) - 1)\n",
    "        ])\n",
    "        # Then, there's a concat step in between\n",
    "        self._conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels= channels[i], out_channels=channels[i+1]) \n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, skip_inputs):\n",
    "        if len(skip_inputs) != len(self._conv_blocks):\n",
    "            raise ValueError(\"Please check implementation. Length of skip inputs and _conv_blocks should be the same!\",\n",
    "                             f\"skip inputs, blocks inputs: {len(skip_inputs), len(self._conv_blocks)}\")\n",
    "        # x is smaller than skip inputs, because there's no padding in the conv layers\n",
    "        for skip_input, up_block, conv_block in zip(skip_inputs, self._upward_conv_blocks, self._conv_blocks):\n",
    "            # print(\"x shape before upsampling: \", x.shape)\n",
    "            x = up_block(x)\n",
    "            # print(skip_input.shape, x.shape)\n",
    "            # TODO: here's a small detail. The paper didn't specify if we want to append or prepend. This might cause trouble\n",
    "            skip_input = self.crop(skip_input=skip_input, x=x)\n",
    "            x = torch.cat((skip_input, x), 1)\n",
    "            # TODO, I'm really not sure if we need to crop. \n",
    "            x = conv_block(x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, skip_input, x):\n",
    "        _, _, H, W = x.shape\n",
    "        return CenterCrop((H,W))(skip_input)\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        encoder_in_channels= [3, 32, 64, 128]    # bottleneck is 128\n",
    "        decoder_channels = [128, 64, 32] #?\n",
    "        self._encoder = Encoder(in_channels=encoder_in_channels)\n",
    "        self._decoder = Decoder(channels=decoder_channels)\n",
    "        # 1x1\n",
    "        self._head = nn.Conv2d(in_channels=decoder_channels[-1], out_channels=class_num, kernel_size=1)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        x, intermediate = self._encoder(x)\n",
    "        output = self._decoder(x, intermediate)\n",
    "        output = self._head(output)\n",
    "        output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "        return output\n",
    "\n",
    "    def _init_weight(self):\n",
    "        with torch.no_grad():\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def forward_pass_poc():\n",
    "    image, target = train_dataset[0]\n",
    "    print(target.shape)\n",
    "    class_num = len(train_dataset.classes)\n",
    "    image = image.unsqueeze(0)\n",
    "    _, _, H, W = image.shape\n",
    "    enc = Encoder([3, 16, 32, 64])\n",
    "    # # print(image.shape)\n",
    "    x, intermediate_outputs = enc.forward(image)\n",
    "    dec = Decoder(channels=[64, 32, 16])\n",
    "    # torch.Size([1, 16, 216, 216])\n",
    "    output = dec(x, intermediate_outputs)\n",
    "    # 1x1\n",
    "    head = nn.Conv2d(\n",
    "        in_channels=16,\n",
    "        out_channels=class_num,\n",
    "        kernel_size=1,\n",
    "    )\n",
    "    output = head(output)\n",
    "    output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "    print(output.shape)\n",
    "forward_pass_poc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rico: predicted test acc 70.9432556500004%\n",
      "tensor([0]) tensor([ 0,  1, 21])\n",
      "Rico: predicted test acc 70.03683435595154%\n",
      "tensor([0]) tensor([ 0,  9, 11, 21])\n",
      "Rico: predicted test acc 73.19779074843441%\n",
      "tensor([0]) tensor([ 0, 19, 21])\n",
      "Rico: predicted test acc 64.70813676279762%\n",
      "tensor([0]) tensor([ 0,  9, 16, 18, 21])\n",
      "Rico: predicted test acc 77.57816696235291%\n",
      "tensor([0]) tensor([ 0,  2, 21])\n",
      "Rico: predicted test acc 74.02743796863264%\n",
      "tensor([0]) tensor([ 0,  7, 10, 15, 21])\n",
      "Rico: predicted test acc 74.92632208384943%\n",
      "tensor([0]) tensor([ 0,  6,  7, 21])\n",
      "Rico: predicted test acc 69.30629351665686%\n",
      "tensor([0]) tensor([ 0,  3, 21])\n",
      "Rico: predicted test acc 76.45517492016228%\n",
      "tensor([0]) tensor([ 0,  8, 16, 21])\n",
      "Rico: predicted test acc 68.10569397054084%\n",
      "tensor([0]) tensor([ 0, 13, 15, 21])\n",
      "Rico: predicted test acc 68.20071602996565%\n",
      "tensor([0]) tensor([ 0, 20, 21])\n",
      "Rico: predicted test acc 70.07979902016217%\n",
      "tensor([0]) tensor([ 0, 13, 21])\n",
      "Rico: predicted test acc 71.58363302118126%\n",
      "tensor([0]) tensor([ 0, 14, 15, 21])\n",
      "Rico: predicted test acc 97.54155495978553%\n",
      "tensor([0]) tensor([ 0, 12, 21])\n",
      "Test Acc: 71.70%\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, test_loader, device, visualize: bool = False):\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        # TODO I AM ITERATING OVER TRAIN_LOADER, SO I'M MORE SURE\n",
    "        for inputs_test, labels_test in test_loader:\n",
    "            inputs_test = inputs_test.to(device)\n",
    "            labels_test = labels_test.to(device)\n",
    "            outputs_test = model(inputs_test)\n",
    "            _, predicted_test = outputs_test.max(1)\n",
    "            mask = (labels_test != 21)\n",
    "            local_total = mask.sum().item()\n",
    "            local_correct = (predicted_test.eq(labels_test) & mask).sum().item()\n",
    "            total_test += local_total\n",
    "            correct_test += local_correct\n",
    "\n",
    "            if visualize:\n",
    "                #TODO Remember to remove\n",
    "                print(f'Rico: predicted test acc {100. * local_correct/local_total}%')\n",
    "                print(torch.unique(predicted_test[0]), torch.unique(labels_test[0]))\n",
    "                # visualize_image_and_target(inputs_test[0], predicted_test[0].unsqueeze(0))\n",
    "\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "    print(f'Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "Epoch [1/70] \n",
      "Epoch [2/70] \n",
      "Epoch [3/70] \n",
      "Epoch [4/70] \n",
      "Epoch [5/70] \n",
      "Epoch [6/70] \n",
      "Epoch [7/70] \n",
      "Epoch [8/70] \n",
      "Epoch [9/70] \n",
      "Epoch [10/70] \n",
      "Epoch [11/70] \n",
      "Epoch [12/70] \n",
      "Epoch [13/70] \n",
      "Epoch [14/70] \n",
      "Epoch [15/70] \n",
      "Epoch [16/70] \n",
      "Epoch [17/70] \n",
      "Epoch [18/70] \n",
      "Epoch [19/70] \n",
      "Epoch [20/70] \n",
      "Epoch [21/70] \n",
      "Epoch [22/70] \n",
      "Epoch [23/70] \n",
      "Epoch [24/70] \n",
      "Epoch [25/70] \n",
      "Epoch [26/70] \n",
      "Epoch [27/70] \n",
      "Epoch [28/70] \n",
      "Epoch [29/70] \n",
      "Epoch [30/70] \n",
      "Epoch [31/70] \n",
      "Epoch [32/70] \n",
      "Epoch [33/70] \n",
      "Epoch [34/70] \n",
      "Epoch [35/70] \n",
      "Epoch [36/70] \n",
      "Epoch [37/70] \n",
      "Epoch [38/70] \n",
      "Epoch [39/70] \n",
      "Epoch [40/70] \n",
      "Epoch [41/70] \n",
      "Epoch [42/70] \n",
      "Epoch [43/70] \n",
      "Epoch [44/70] \n",
      "Epoch [45/70] \n",
      "Epoch [46/70] \n",
      "Epoch [47/70] \n",
      "Epoch [48/70] \n",
      "Epoch [49/70] \n",
      "Epoch [50/70] \n",
      "Epoch [51/70] \n",
      "Epoch [52/70] \n",
      "Epoch [53/70] \n",
      "Epoch [54/70] \n",
      "Epoch [55/70] \n",
      "Epoch [56/70] \n",
      "Epoch [57/70] \n",
      "Epoch [58/70] \n",
      "Epoch [59/70] \n",
      "Epoch [60/70] \n",
      "Epoch [61/70] \n",
      "Epoch [62/70] \n",
      "Epoch [63/70] \n",
      "Epoch [64/70] \n",
      "Epoch [65/70] \n",
      "Epoch [66/70] \n",
      "Epoch [67/70] \n",
      "Epoch [68/70] \n",
      "Epoch [69/70] \n",
      "Epoch [70/70] \n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from torch import optim\n",
    "\n",
    "# Define the training function\n",
    "MODEL_PATH = 'unet_pascal.pth'\n",
    "ACCUMULATION_STEPS = 8\n",
    "\n",
    "# Check against example\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        start = time.time()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] ')\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        #     inputs = inputs.to(device)\n",
    "        #     labels = labels.to(device)\n",
    "        #     # Forward pass\n",
    "        #     outputs = model(inputs)\n",
    "        #     # This is because torch.nn.CrossEntropyLoss(reduction='mean') is true, so to simulate a larger batch, we need to further divide\n",
    "        #     # print(f\"output: {outputs.dtype}, labels: {labels.dtype}\")\n",
    "        #     loss = criterion(outputs, labels)/ACCUMULATION_STEPS\n",
    "        #     # Backward pass and optimization\n",
    "        #     loss.backward()\n",
    "        #     if (i+1)%ACCUMULATION_STEPS == 0:\n",
    "        #         optimizer.step()\n",
    "        #         # Zero the parameter gradients\n",
    "        #         optimizer.zero_grad()\n",
    "        #         # break #TODO\n",
    "        #     # Statistics\n",
    "        #     running_loss += loss.item() * inputs.size(0)\n",
    "        #     _, predicted = outputs.max(1)\n",
    "        #     # print(predicted.shape)\n",
    "        #     mask = (labels != 21)\n",
    "        #     total_train += mask.sum().item()\n",
    "        #     # print((predicted == labels).sum().item(), ((predicted == labels) & mask).sum().item())\n",
    "        #     # print(mask.sum().item())\n",
    "            \n",
    "        #     correct_train += ((predicted == labels) & mask).sum().item()\n",
    "\n",
    "        # # adjust after every epoch\n",
    "        # scheduler.step()  # TODO: disabled for Adam optimizer\n",
    "        # current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Current learning rate: {current_lr}\")\n",
    "        # epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        # epoch_acc = 100. * correct_train / total_train\n",
    "        # print(\"correct train: \", correct_train, \" total train: \", total_train)\n",
    "        # end = time.time()\n",
    "        \n",
    "        # print(\"elapsed: \", end-start)\n",
    "\n",
    "        # torch.save(model.state_dict(), MODEL_PATH)\n",
    "        # print(f\"epoch: {epoch}, saved the model. \"\n",
    "        #       f'Train Loss: {epoch_loss:.4f} '\n",
    "        #       f'Train Acc: {epoch_acc:.2f}% ')\n",
    "    # eval_model(model, test_loader=test_dataloader, device=device) \n",
    "    print('Training complete')\n",
    "    return model\n",
    "\n",
    "model = UNet(class_num = len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "weight_decay = 0.0001\n",
    "# momentum=0.9\n",
    "learning_rate=0.001\n",
    "num_epochs=70\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 40], gamma=0.1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=False, map_location=device))\n",
    "    print(\"loaded model\")\n",
    "model.to(device)\n",
    "\n",
    "model = train_model(model, train_dataloader, test_dataloader, criterion, optimizer, scheduler,\n",
    "                    num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: _encoder._layers.0.conv1.weight | Average Weight: -0.001730\n",
      "Layer: _encoder._layers.0.conv2.weight | Average Weight: 0.000583\n",
      "Layer: _encoder._layers.1.conv1.weight | Average Weight: 0.000499\n",
      "Layer: _encoder._layers.1.conv2.weight | Average Weight: -0.000026\n",
      "Layer: _encoder._layers.2.conv1.weight | Average Weight: 0.000001\n",
      "Layer: _encoder._layers.2.conv2.weight | Average Weight: 0.000086\n",
      "Layer: _decoder._upward_conv_blocks.0.weight | Average Weight: -0.000054\n",
      "Layer: _decoder._upward_conv_blocks.1.weight | Average Weight: 0.000328\n",
      "Layer: _decoder._conv_blocks.0.conv1.weight | Average Weight: -0.000004\n",
      "Layer: _decoder._conv_blocks.0.conv2.weight | Average Weight: -0.000198\n",
      "Layer: _decoder._conv_blocks.1.conv1.weight | Average Weight: 0.000534\n",
      "Layer: _decoder._conv_blocks.1.conv2.weight | Average Weight: -0.000918\n",
      "Layer: _head.weight | Average Weight: 0.001126\n",
      "Overall Average Weight in the Network: 0.000044\n"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (3, 356, 356), summary_mode=True)\n",
    "\n",
    "def calculate_average_weights(model):\n",
    "    total_sum = 0\n",
    "    total_elements = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            weight_mean = param.mean().item()\n",
    "            total_sum += param.sum().item()\n",
    "            total_elements += param.numel()\n",
    "            print(f\"Layer: {name} | Average Weight: {weight_mean:.6f}\")\n",
    "    \n",
    "    overall_average = total_sum / total_elements if total_elements > 0 else 0\n",
    "    print(f\"Overall Average Weight in the Network: {overall_average:.6f}\")\n",
    "\n",
    "# calculate_average_weights(model)\n",
    "\n",
    "eval_model(model, test_dataloader, device=device, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
