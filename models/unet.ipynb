{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notes:\n",
    "- Technically, UNet can be used on images of various input images. Would you recommend doing the same transforms? \n",
    "- What does torch.modulelist do? What is it similar to in tensor flow?\n",
    "    - An `nn.module` is usually a layer / group of layers. `torch.modulelist` is a container for that, and it can be indexed like a regular list. But in the meantime these are layers are registered in the model for calls like `model.to()`, `model.train()`, `model.eval()`\n",
    "\n",
    "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d) - TODO\n",
    "    ```python\n",
    "    conv_transpose = nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=2, stride=2, padding=0)\n",
    "    input_tensor = torch.randn(1, 3, 5, 5)  # 1 batch, 3 channels, 5x5 image\n",
    "    output_tensor = conv_transpose(input_tensor)\n",
    "    output_tensor.shape\n",
    "    ```\n",
    "- How does concat work? horizontally: `torch.cat((x, x, x), 1)`, vertically: `torch.cat((x, x, x), 0)`\n",
    "\n",
    "- How to get model summary in pytorch? \n",
    "```\n",
    "pip install torchsummary\n",
    "from torchsummary import summary\n",
    "# Assuming `model` is your neural network\n",
    "summary(model, input_size=(3, 224, 224))  # For an input image of size 224x224 with 3 channels (RGB)\n",
    "```\n",
    "- Here, we need to do `crop()` actually. The best way is `from torchvision.transforms import CenterCrop`\n",
    "\n",
    "- increase dimension at dim=0: `new_t = t.unsqueeze(0)`\n",
    "\n",
    "- What do you do when the final output is `torch.Size([1, 16, 216, 216])`, while your input is `torch.Size([1, 3, 256, 256])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rico/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2, CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from functools import cached_property\n",
    "\n",
    "DATA_DIR='./data'\n",
    "\n",
    "def replace_tensor_val(tensor, a, b):\n",
    "    tensor[tensor==a] = b\n",
    "    return tensor\n",
    "\n",
    "image_seg_transforms = transforms.Compose([\n",
    "   v2.Resize((256, 256)),\n",
    "   v2.ToTensor(),\n",
    "   v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_seg_transforms = transforms.Compose([\n",
    "    v2.Resize((256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "    v2.PILToTensor(),\n",
    "    v2.Lambda(lambda x: replace_tensor_val(x.long(), 255, 21)),\n",
    "])\n",
    "\n",
    "class VOCSegmentationClass(Dataset):\n",
    "   def __init__(self, image_set): \n",
    "        # Load PASCAL VOC 2007 dataset for segmentation\n",
    "        self._dataset = datasets.VOCSegmentation(\n",
    "            root=DATA_DIR,  # Specify where to store the data\n",
    "            year='2007',    # Specify the year of the dataset (2007 in this case)\n",
    "            image_set=image_set,  # You can use 'train', 'val', or 'trainval'\n",
    "            download=True,  # Automatically download if not available\n",
    "            transform=image_seg_transforms,  # Apply transformations to the images\n",
    "            target_transform=target_seg_transforms  # Apply transformations to the masks\n",
    "        )\n",
    "        self._classes = set()\n",
    "   @cached_property\n",
    "   def classes(self):\n",
    "       if len(self._classes) == 0: \n",
    "           for image, target in self._dataset:\n",
    "            self._classes.update(torch.unique(target).tolist())\n",
    "       return self._classes\n",
    "   def __getitem__(self, index): \n",
    "       # return an image and a label. In this case, a label is an image with int8 values\n",
    "       return self._dataset[index]\n",
    "       # TODO: more transforms?\n",
    "   def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "train_dataset = VOCSegmentationClass(image_set='train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 16,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_dataset = VOCSegmentationClass(image_set='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 16,\n",
    "    shuffle=True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"classes: \", train_dataset.classes)\n",
    "# for image, target in train_dataset:\n",
    "# #     # To see what our data looks like\n",
    "# #     # # See torch.Size([3, 281, 500]) torch.Size([1, 281, 500])\n",
    "#     # # print(image.shape, target.shape)\n",
    "\n",
    "# #     plt.subplot(1,2,1)\n",
    "# #     # Making channels the last dimension\n",
    "# #     plt.imshow(image.permute(1,2,0))\n",
    "# #     plt.title('image')\n",
    "\n",
    "# #     plt.subplot(1,2,2)\n",
    "# #     # Making channels the last dimension\n",
    "# #     plt.imshow(target.permute(1,2,0))\n",
    "# #     plt.title('mask')\n",
    "# #     # See tensor([  0,   1,  15, 255], dtype=torch.uint8)\n",
    "# #     print(\"unique: \", torch.unique(target[0]))\n",
    "# #     plt.show()\n",
    "# #     break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a regular conv block\n",
    "from torch import nn as nn\n",
    "from collections import deque\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # This should include the bottleneck.\n",
    "        self._layers = nn.ModuleList([ConvBlock(in_channels[i], in_channels[i+1]) for i in range(len(in_channels) - 1)])\n",
    "        self._maxpool = nn.MaxPool2d(2, stride=2)\n",
    "    def forward(self, x):\n",
    "        # returns unpooled output from each block: \n",
    "        # [intermediate results ... ], but we don't want to return \n",
    "        intermediate_outputs = deque([])\n",
    "        for i in range(len(self._layers) - 1):\n",
    "            x = self._layers[i](x)\n",
    "            intermediate_outputs.appendleft(x)\n",
    "            x = self._maxpool(x)\n",
    "        x = self._layers[-1](x)\n",
    "        return x, intermediate_outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self._upward_conv_blocks = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels = channels[i], out_channels = channels[i+1], \n",
    "                kernel_size=2, stride=2\n",
    "            ) for i in range(len(channels) - 1)\n",
    "        ])\n",
    "        # Then, there's a concat step in between\n",
    "        self._conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels= channels[i], out_channels=channels[i+1]) \n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, skip_inputs):\n",
    "        if len(skip_inputs) != len(self._conv_blocks):\n",
    "            raise ValueError(\"Please check implementation. Length of skip inputs and _conv_blocks should be the same!\",\n",
    "                             f\"skip inputs, blocks inputs: {len(skip_inputs), len(self._conv_blocks)}\")\n",
    "        # x is smaller than skip inputs, because there's no padding in the conv layers\n",
    "        for skip_input, up_block, conv_block in zip(skip_inputs, self._upward_conv_blocks, self._conv_blocks):\n",
    "            print(\"x shape before upsampling: \", x.shape)\n",
    "            x = up_block(x)\n",
    "            print(skip_input.shape, x.shape)\n",
    "            # TODO: here's a small detail. The paper didn't specify if we want to append or prepend. This might cause trouble\n",
    "            skip_input = self.crop(skip_input=skip_input, x=x)\n",
    "            x = torch.cat((skip_input, x), 1)\n",
    "            # TODO, I'm really not sure if we need to crop. \n",
    "            x = conv_block(x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, skip_input, x):\n",
    "        _, _, H, W = x.shape\n",
    "        return CenterCrop((H,W))(skip_input)\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        encoder_in_channels= [3, 16, 32, 64]    # bottleneck is 128\n",
    "        decoder_channels = [64, 32, 16] #?\n",
    "        self._encoder = Encoder(in_channels=encoder_in_channels)\n",
    "        self._decoder = Decoder(channels=decoder_channels)\n",
    "        # 1x1\n",
    "        self._head = nn.Conv2d(in_channels=decoder_channels[-1], out_channels=1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x, intermediate = self._encoder(x)\n",
    "        output = self._decoder(x, intermediate)\n",
    "        output = self._head(output)\n",
    "        output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')\n",
    "        return output\n",
    "\n",
    "def forward_pass_poc():\n",
    "    image, _ = train_dataset[0]\n",
    "    image = image.unsqueeze(0)\n",
    "    _, _, H, W = image.shape\n",
    "    enc = Encoder([3, 16, 32, 64])\n",
    "    # # print(image.shape)\n",
    "    x, intermediate_outputs = enc.forward(image)\n",
    "    dec = Decoder(channels=[64, 32, 16])\n",
    "    # torch.Size([1, 16, 216, 216])\n",
    "    output = dec(x, intermediate_outputs)\n",
    "    # 1x1\n",
    "    head = nn.Conv2d(\n",
    "        in_channels=16,\n",
    "        out_channels=1,\n",
    "        kernel_size=1,\n",
    "    )\n",
    "    output = head(output)\n",
    "    output = torch.nn.functional.interpolate(output, size=(H,W),  mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- do I need scheduler if I'm changing step size? Optional\n",
    "- What is BCEWithLogitsLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\n\u001b[0;32m---> 26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:73\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     62\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     63\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:362\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    360\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    364\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import optim\n",
    "\n",
    "# Define the training function\n",
    "MODEL_PATH = 'unet_pascal.pth'\n",
    "ACCUMULATION_STEPS = 8\n",
    "# Check against example\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        start = time.time()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] ')\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "weight_decay = 0.0001\n",
    "# momentum=0.9\n",
    "learning_rate=0.001\n",
    "num_epochs=50\n",
    "batch_size=16\n",
    "model = UNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
